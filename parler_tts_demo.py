# import torch
# # from parler_tts import ParlerTTSForConditionalGeneration
# # from ParlerTTS.parler_tts import ParlerTTSForConditionalGeneration
# # import parler_tts
# from parler_tts.modeling_parler_tts import ParlerTTSForConditionalGeneration
# from transformers import AutoTokenizer, set_seed, AutoFeatureExtractor
# import soundfile as sf
# import torchaudio
# import uuid

# device = "cuda" if torch.cuda.is_available() else "cpu"

# model_id = "parler-tts/parler-tts-mini-v1"

# # model = ParlerTTSForConditionalGeneration.from_pretrained(model_id, cache_dir="/media/mirae/NewVolume/huggingface/models", attn_implementation='sdpa').to(device)

# model = ParlerTTSForConditionalGeneration.from_pretrained(model_id, cache_dir="/media/mirae/NewVolume/huggingface/models").to(device)
# tokenizer = AutoTokenizer.from_pretrained(model_id)
# feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

# # audio_path = "/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/parler_tts_out974873b2-7f1a-4a08-b81b-660c19b20c20.wav"

# audio_path = "/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/raveena.mp3"
# init_audio, init_sr = torchaudio.load(audio_path)
# init_audio = torchaudio.functional.resample(init_audio, init_sr, model.config.sampling_rate)
# init_audio = init_audio.mean(0)
# # init_prompt = "Here, write the transcript of the init audio"
# init_prompt = "I found that Parler has difficulty generalizing to unseen speakers (meaning using a speaker that has not been seen during training or that has not been generated by Parler), so there's no actual edge of using it for voice cloning. However, from my experiment, it's working quite well with Parler generation!"
# prompt = "Is it really working ?"
# # description = "A woman speaker speaks slowly with a low-pitched voice. The recording is of very high quality, with the speaker's voice sounding clear and very close up." # TODO: adapt the prompt to describe the input audio

# # prompt = "I found that Parler has difficulty generalizing to unseen speakers (meaning using a speaker that has not been seen during training or that has not been generated by Parler), so there's no actual edge of using it for voice cloning. However, from my experiment, it's working quite well with Parler generation!"
# # description = "Here, write the transcript of the init audio"
# # prompt = "Here, write the transcript of the init audio"
# description = "A woman speaks quickly with a low-pitched voice. The recording is of high quality, with the speaker's voice sounding clear and very close up"

# # description = "Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise."
# input_ids = tokenizer(description, return_tensors="pt").input_ids.to(device)
# prompt_input_ids = tokenizer(init_prompt + " " + prompt, return_tensors="pt").input_ids.to(device)
# input_values = feature_extractor(init_audio, sampling_rate=model.config.sampling_rate, return_tensors="pt").input_values.to(device)

# # prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

# set_seed(2)

# import ipdb; ipdb.set_trace()

# # invoke generate with input_values created using init_audio   
# generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids, input_values = input_values)

# # generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids, reference_voice="raveena.mp3")

# # generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)
# audio_arr = generation.cpu().numpy().squeeze()
# unique_id = uuid.uuid4()
# unique_id = str(unique_id)

# sf.write("parler_tts_out"+unique_id+".wav", audio_arr, model.config.sampling_rate)



import torch
from parler_tts import ParlerTTSForConditionalGeneration
from transformers import AutoTokenizer, set_seed, AutoFeatureExtractor
import soundfile as sf
import torchaudio
# from parler_tts.reference_voice_encoder import ReferenceVoiceEncoder

device = "cuda" if torch.cuda.is_available() else "cpu"

model_id = "parler-tts/parler-tts-large-v1"

model = ParlerTTSForConditionalGeneration.from_pretrained(model_id, cache_dir="/media/mirae/NewVolume/huggingface/models").to(device)
tokenizer = AutoTokenizer.from_pretrained(model_id)
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id)

# init_audio, init_sr = torchaudio.load("/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/parler_tts_out3cce1fb6-cbb6-42a3-83b9-3c69b45b2399.wav")

# init_audio, init_sr = torchaudio.load("/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/raveena_new.mp3")
# init_audio = torchaudio.functional.resample(init_audio, init_sr, model.config.sampling_rate)
# init_audio = init_audio.mean(0)
# init_prompt = "Here, write the transcript of the init audio"

prompt = "Is it really working or is this some false positive results?"
# description = "A woman speaks quickly with a low-pitched voice. The recording is of high quality, with the speaker's voice sounding clear and very close up" # TODO: adapt the prompt to describe the input audio
description = "A female voice with an American accent enunciates every word with precision. The speaker's voice is very close-sounding, and the recording is excellent, capturing her voice with crisp clarity. Her tone is fairly expressive and animated."
input_ids = tokenizer(description, return_tensors="pt").input_ids.to(device)
# prompt_input_ids = tokenizer(init_prompt + " " + prompt, return_tensors="pt").input_ids.to(device)
prompt_input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

import ipdb; ipdb.set_trace()
# input_values = feature_extractor(init_audio, sampling_rate=model.config.sampling_rate, return_tensors="pt").input_values.to(device)

# reference_voice_encoder =  ReferenceVoiceEncoder( weights_fpath="/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/parler_tts/speaker_encoder.pt", device="cuda", eval=True, verbose=False)
# test_input_values, _ = reference_voice_encoder.embed_utterance_from_file(fpath="/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/raveena_new.mp3",numpy=False)
# test_input_values = test_input_values.unsqueeze(0).unsqueeze(0)
# import torch.nn.functional as F

# test_input_values = F.pad(test_input_values, (0, 16, 0, 0)) 
set_seed(2)

generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids, reference_speaker="/media/mirae/NewVolume/Projects/ParlerTTS/parler_tts/raveena_new.mp3")
# generation = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids, input_values = input_values)
audio_arr = generation.cpu().numpy().squeeze()
sf.write("parler_tts_out_raveena_4.wav", audio_arr, model.config.sampling_rate)